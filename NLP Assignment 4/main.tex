\documentclass[a4paper,12pt]{ETHexercise}
\usepackage{bbm}

\input{preamble}

\usepackage{multirow}
\title{NLP Assignment}
\begin{document}
\setserie{1}



\lectureheader{Prof. Ryan Cotterell}
{}
{\Large Natural Language Processing}{Fall 2022}
\begin{center}
    {\Huge Yannick Wattenberg: Assignment 04}\\
      \quad\newline
      ywattenberg@inf.ethz.ch, 19-947-464.\\
      \quad\newline
    \timestamp

\end{center}
\section*{Question 1: Calculating Prefix Probabilities}
\subsection*{a)}
We prove by 
The following equality is given
\begin{align}
    \sum_{w \in \Sigma} p(w) = 1 \label{eq:g1}
\end{align}
We first prove the following equivalence:
\begin{align}
    \sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w) = 1
\end{align}
Using induction over n and the above equation as induction hypothesis.
\begin{align}
    \sum_{w \in \Sigma^*, |w| = 1} \tilde{p}(w) &= \sum_{w \in \Sigma} p(w) = 1\\
    n &\rightarrow n+1 \nonumber\\
    \sum_{w \in \Sigma^*, |w| = n+1} \tilde{p}(w) &=  \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w' \circ w)\\
    &= \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} p(w')\tilde{p}(w) &&\text{def. }\tilde{p}\\
    &= \sum_{w' \in \Sigma}  p(w') \cdot \sum_{w \in \Sigma^*, |w| = n}\tilde{p}(w)\\
    &= \sum_{w' \in \Sigma}  p(w') &&(IH)\\
    &= 1 &&\ref{eq:g1}
\end{align}

With this we can reformulate the sum as follows:
\begin{align}
    \sum_{w \in \Sigma^*} \tilde{p}(w) &= \sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\\
    &=   \sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\\
    &=   \sum_{i=0}^{\infty} 1\\
     \sum_{i=1}^{\infty} 1 &\rightarrow \infty
\end{align}

\subsection*{b)}
The following equality is given
\begin{align}
    \sum_{w \in \Sigma \cup EOS} p(w) = 1 \Rightarrow \sum_{w \in \Sigma} p(w) = 1 - p(EOS) \label{eq:g2}
\end{align}
We first prove the following equivalence:
\begin{align}
    \sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w) = (1 - p(EOS))^n
\end{align}
Using induction over n and the above equation as induction hypothesis.
\begin{align}
    \sum_{w \in \Sigma^*, |w| = 1} \tilde{p}(w) &= \sum_{w \in \Sigma} p(w) = 1 - p(EOS) &&\ref{eq:g2} \\
    n &\rightarrow n+1 \nonumber\\
    \sum_{w \in \Sigma^*, |w| = n+1} \tilde{p}(w) &=  \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w' \circ w)\\
    &= \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} p(w')\tilde{p}(w) &&\text{def. }\tilde{p}\\
    &= \sum_{w' \in \Sigma}  p(w') \cdot \sum_{w \in \Sigma^*, |w| = n}\tilde{p}(w)\\
    &= \sum_{w' \in \Sigma}  p(w') \cdot (1-p(EOS))^n &&(IH)\\
    &=  (1-p(EOS)) \cdot (1-p(EOS))^n &&\ref{eq:g2}\\
    &= (1-p(EOS))^{n+1}
\end{align}

With this we can reformulate the sum as follows:
\begin{align}
    \sum_{w \in \Sigma^*} p(w) = \sum_{w \in \Sigma^*} p(EOS) \tilde{p}(w)  &= p(EOS) \sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\\
    &= p(EOS)  \left(\sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\right)\\
    &= p(EOS)  \left(\sum_{i=0}^{\infty} (1-p(EOS))^{i} \right)\\
     p(EOS) \left(  \sum_{i=0}^{\infty} (1-p(EOS))^{i} \right) &\rightarrow p(EOS)  \frac{1}{p(EOS)} = 1 \quad (\text{geom. series})
\end{align}

\subsection*{c)}
\begin{align}
    p_{pre}(w) &\stackrel{!}{=} \sum_{u\in\Sigma^*}p(wu)\\
    \sum_{u\in\Sigma^*}p(wu) &= \sum_{u \in \Sigma^*}p(EOS|wu)p_{pre}(u|w)p_{pre}(w)\\
    &= p_{pre}(w) \left(\sum_{u \in \Sigma^*}p(EOS|wu)p_{pre}(u|w)\right)\\
    &= \frac{1}{p(EOS|w)} p(w) \left(\sum_{u \in \Sigma^*}p(u|w)\right) &&(\text{def.} P)\\
    &= \frac{1}{p(EOS|w)} \left(\sum_{u \in \Sigma^*}p(w)\frac{p(w|u)p(u)}{p(w)}\right) &&(\text{Bayes.}) \label{eq:bayes}\\
    &= \frac{1}{p(EOS|w)} \left(\sum_{u \in \Sigma^*}p(w|u)p(u)\right) \\
    &= \frac{1}{p(EOS|w)} p(w) &&(\text{Bayes.})\\
    &= p_{pre}(w) &&(\text{def.} P)
\end{align}
In (\ref{eq:bayes}) we apply baysens rule which holds as the sum iterates over all possible suffixes. This means we sum over the whole probability space which gives probability one for the event we condition on.

\subsection*{d)}
One can use the normal CKY algorithm where we define the score-function as the natural logarithm of the probability of the applied rule.
This means the CKY algorithm will calculate
\begin{align}
   chart[i,k,X] \mathrel{+}= &exp(score(X\rightarrow Y Z)) \cdot chart[i,j,Y] \cdot chart[j,k,Z] \\
    = &p( Y Z| X) \cdot p(Y|w_i,...,w_j) \cdot p(Z|w_j,...,w_k)\\
    \Rightarrow chart[i,k,X] = &p(X|w_i,...,w_k)
\end{align}
Which gives us $p(w_1,...,w_n|S)$ for the top most cell. This is the Prefix probability of the sentence we can multiply this by $p(EOS|S)$ to get the probability of the sentence.

\subsection*{e)}
\begin{align}
    \sum_{u \in \Sigma^*}p(wu) &\stackrel{!}{=} p(S \stackrel{*}{\Rightarrow} wv)\\
    \sum_{u \in \Sigma^*}p(wu) &= \sum_{u \in \Sigma^*} p_{inside}(wu|S) &&(\text{S is the starting symbol})\\
    &= \sum_{u \in \Sigma^*} p(S \stackrel{*}{\Rightarrow} wu) &&(\text{def. }p_{inside})\\
    &= p(S \stackrel{*}{\Rightarrow} wv) &&(\text{for some arbitrary }v \in \Sigma^*)
\end{align}
In the last step we replace the sum by some arbitrary suffix $v$. This is possible as the sum iterates over all possible suffixes and ($\stackrel{!}{\Rightarrow}$) produces all possible derivations meaning it also generates all possible suffixes.

\subsection*{f)}
First we define the probability matrix $W$ of size $|\mathcal{N}| \times |\mathcal{N}|$ as follows: Every entry $W[A,B]$ gets the probability of $p[A \Rightarrow B\alpha]$ where $\alpha$ is the rest of the rule. We can calculate this probability by adding the probabilities of all rules which produce $B$ as a first symbol. There can be at most $|\mathcal{N}|$ such rules as our grammar is in Chomsky normal form.
This means it is possible to calculate all entries of the matrix $W$ in $\mathcal{O}(|\mathcal{N}|^3)$.

Now we have to calculate $W^*$. We can do this using Lehmann's algorithm as seen in the lecture notes. For this to work it has to hold that entries on the diagonal of $W$ are smaller than one, otherwise $W[A,A]^*$ would be infinity/diverge. However all entries on the diagonal have to be smaller than one as otherwise this would mean all derivations from $A$ produce another $A$ thus every sentence would be infinite, which we assume to not be the case. So we can use Lehmann's algorithm to calculate $W^*$ which runs in $\mathcal{O}(|\mathcal{N}|^3)$.
Now the entry $W^*[A,B]$ is the probability of $p[A  \stackrel{*}{\Rightarrow} B\alpha] = p_{lc}(B|A)$. This follows from the definition of $W^*$ and the definition of the matrix product. We already proved in Assignment 3 exercise 2 b) that $M^n$ encodes the sum of path lengths of length $n$ in the graph $M$, this translates to our example where the path length will be the probabilities of a specific derivation path.
Then it follows from the definition of $W^*$ that $W^*[A,B]$ is the sum of probabilities of a derivation path from $A$ to $B$ of length $n$ for $n$ from $0$ to $\infty$. This is the same as $p_{lc}(B|A)$.
Now with all $p_{lc}(B|A)$ calculated we can calculate $p_{lc}(YZ|X) = \sum_{X' \in \mathcal{N}} p_{lc}(X'|X)p(X' \Rightarrow YZ)$ which is in $\mathcal{O}(|\mathcal{N}|)$. Doing this for all entries will take $\mathcal{O}(|\mathcal{N}|^4)$ as we have to calculate the sum for all possible $X$, $Y$ and $Z$.

\subsection*{g)}
\begin{align*}
    p_{pre}(w_i,...,w_k|X) \stackrel{!}{=} \sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)\cdot p_{inside}(w_i,...,w_j|Y)\cdot p_{pre}(w_j+1,...,w_k|Z)
\end{align*}
\begin{align}
    &\sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)\cdot p_{inside}(w_i,...,w_j|Y)\cdot p_{pre}(w_j+1,...,w_k|Z)\\
    =&\sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p(X \stackrel{*}{\Rightarrow} YZ\alpha)\cdot p(Y \stackrel{*}{\Rightarrow} w_i,...,w_j)\cdot p(Z \stackrel{*}{\Rightarrow} w_{j+1},...,w_k,\textbf{v})\\
\end{align}
Further we have:
\begin{align}
    &p(X \stackrel{*}{\Rightarrow} Y'Z'\alpha)\cdot p(Y' \stackrel{*}{\Rightarrow} w_i,...,w_{j'})\cdot p(Z' \stackrel{*}{\Rightarrow} w_{j'+1},...,w_k,\textbf{v'})\\
    = &p(X \stackrel{*}{\Rightarrow} w_i,...,w_k,\textbf{v}) \label{eq:big}\\
    - &\left( \sum_{j=1, j \neq j'}^{k-1}\sum_{Y,Z\in\mathcal{N}; Y\neq Y', Z \neq Z' }p(X \stackrel{*}{\Rightarrow} YZ\alpha)\cdot p(Y \stackrel{*}{\Rightarrow} w_i,...,w_j)\cdot p(Z \stackrel{*}{\Rightarrow} w_{j+1},...,w_k,\textbf{v}) \right) \\
    \Rightarrow &\sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p(X \stackrel{*}{\Rightarrow} YZ\alpha)\cdot p(Y \stackrel{*}{\Rightarrow} w_i,...,w_j)\cdot p(Z \stackrel{*}{\Rightarrow} w_{j+1},...,w_k,\textbf{v})\\
    = &p(X \stackrel{*}{\Rightarrow} w_i,...,w_k,\textbf{v}) \\
    =&p_{pre}(w_i,...,w_k|X)
\end{align}
The important step here is equality of (\ref{eq:big}) and the previous equation. Which is valid as $p(X \stackrel{*}{\Rightarrow} w_i,...,w_k,\textbf{v})$ is the sum of the probabilities of all derivation trees of $w_i, ..., w_k,,\textbf{v} $. Further the previous equation is the probability of deriving $w_i,...,w_j$ from Y', $w_i,...,w_k,\textbf{v}$ from Z' and Z'Y'$\alpha$ from X. This corresponds to the probabilities of the trees seen in figure 1: b).  Thus in (\ref{eq:big}) we calculate the difference between all possible derivation trees and the derivation trees that we are not intrested in i.e. the ones that do not the ones just mentioned. 

\subsection*{h)}
Using f) we can calculate the left corner probabilities in $\mathcal{O}(|\mathcal{N}|^4)$. After that we can calculate $p_{pre}(w_k|X)$ as follows:
\begin{align}
    p_{pre}(w_k|X) = p_{pre}(X \stackrel{*}{\Rightarrow} w_k) = \sum_{Y \in \mathcal{N}}p_{lc}(Y|X)p(Y \Rightarrow w_k)
\end{align}
This is possible in $\mathcal{O}(|\mathcal{N}|)$ as we can have at most one rule per nonterminal that derives the terminal $w_k$. Thus we can calculate $p_{pre}(w_k|X)$ for all $X \in \mathcal{N}$ in $\mathcal{O}(|\mathcal{N}|^2)$. 
We can also compute the inside probabilities for all substrings of $w_i,...,w_{k-1}$ and tags in $\mathcal{O}(N^3|\mathcal{R}|) = \mathcal{O}(N^3|\mathcal{N}|^3)$ using the CKY algorithm with the score function defined as the $ln$ of probability of the derivation rule. We also return the whole chart instead of just $chart[1,N+1,S]$. Thus in $chart[j,i,X]$ we have the probability of deriving $w_j,...,w_i$ from $X$. 

Now we can calculate the prefix probabilities $p_{pre}(w_l, w_k | X)$ for all X for $l$ from $k-1$ to $i$. We have all probabilities in the sums precomputed for $l = k-1$. For each we need to iterate over all $j$ in range $[l,k-1]$ and all $Y,Z$. The outer sum iterates over at most $N$ elements while the inner sum iterates over less than $|N|^2$ elements. Thus the entire sum will take at most $\mathcal{O}(N|\mathcal{N}|^2)$ to calculate. Calculating for all X gives us a runtime of $\mathcal{O}(N|\mathcal{N}|^2)$. From this follows that the whole suppart runs in $\mathcal{O}(N^2|\mathcal{N}|^3)$ as we need to compute this sum N times.

Then the whole algorithm runs in $\mathcal{O}(N^2|\mathcal{N}|^3) + \mathcal{O}(N^3|\mathcal{N}|^3) + \mathcal{O}(|\mathcal{N}|^4) = \mathcal{O}(\mathcal{O}(N^3|\mathcal{N}|^3) + \mathcal{O}(|\mathcal{N}|^4))$

Calling this algorithm for all prefixes of $\textbf{w}$ then gives a runtime of $\mathcal{O}(N^4|\mathcal{N}|^3 + N|\mathcal{N}|^4)$.

\subsection*{i)}
To make the previous algorithm more efficient we can instead use the CKY algorithm to calculate the prefix probabilities. The precomputations are the same as before, we calculate the left corner probabilities, the inside probabilities. We also now have to precompute the prefix probabilities for all of the terminals in $\textbf{w}$ thus instead of $\mathcal{O}(|\mathcal{N}|^2)$ we get $\mathcal{O}(N |\mathcal{N}|^2)$.
Next we use the CKY algorithm to calculate the rest of the prefix probabilities, $chart[n,n+1, X]$ will be initialized to $p_{pre}(w_n|X)$. Then given the previous levels of prefix probabilities we calculate the current level as follows:
\begin{align}
    chart[n, l, X] = \sum_{j=i}^{l-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)p_{inside}(w_n,...,w_j|Y)chart[j+1,l,Z]
\end{align}
It is obvious that $chart[n, l, X] = p_{pre}(w_n,...,w_l|X)$. We can prove this by strong induction. The base case of $l = n+1$ is trivial as we initialized the chart to hold the prefix probabilities. For our induction assumption we assume that $chart[i, j, X] = p_{pre}(w_i,...,w_j|X)$ for all $i,j; j - i < l - n$ and all $X \in \mathcal{N}$. Then we can calculate $chart[n, l, X]$ as stated:
\begin{align}
    chart[n, l, X] &= \sum_{j=i}^{l-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)p_{inside}(w_n,...,w_j|Y)chart[j+1,l,Z]\\
    &= \sum_{j=i}^{l-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)p_{inside}(w_n,...,w_j|Y)p_{pre}(w_{j+1},...,w_l|Z) &&(\text{IH})\\
    &= p_{pre}(w_n,...,w_l|X) &&(\text{def }p_{pre})
\end{align}
Then as we have used the CKY algorithm the runtime is $\mathcal{O}(N^3|\mathcal{N}|^3)$.
This gives us a runtime of $\mathcal{O}(N^3|\mathcal{N}|^3 + |\mathcal{N}|^4)$.

\subsection*{j)}
\begin{align}
    p_{pre}(w) &= \prod_{n=1}^N p(w_n|w_0,...,w_{n-1})\\
    \Rightarrow p_{pre}(w_j|w_0,...,w_{j-1}) &= 
    \frac{p_{pre}(w)}{\prod_{n=1, n \neq j}^N p(w_n|w_0,...,w_{n-1})}\\
    &=\frac{p_{pre}(w)}{p_{pre}(w_0,...,w_{j-1}) \frac{p_{pre}(w)}{p_{pre}(w_0,...,w_{j+1})}}\\
    &= \frac{p(S \stackrel{*}{\Rightarrow} \textbf{wv})}{p(S \stackrel{*}{\Rightarrow} w_0,...,w_{j-1}\textbf{v}) \frac{p(S \stackrel{*}{\Rightarrow} \textbf{wv})}{p(S \stackrel{*}{\Rightarrow} w_0,...,w_{j+1}\textbf{v})}}\\
    &= \frac{p(S \stackrel{*}{\Rightarrow} w_0,...,w_{j+1}\textbf{v})}{p(S \stackrel{*}{\Rightarrow} w_0,...,w_{j-1}\textbf{v})}\\
    &= \frac{p_{pre}(w_0,...,w_{j+1})}{p_{pre}(w_0,...,w_{j-1})}
\end{align}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
