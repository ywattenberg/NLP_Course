\documentclass[a4paper,12pt]{ETHexercise}
\usepackage{bbm}

\input{preamble}

\usepackage{multirow}
\title{NLP Assignment}
\begin{document}
\setserie{1}



\lectureheader{Prof. Ryan Cotterell}
{}
{\Large Natural Language Processing}{Fall 2022}
\begin{center}
    {\Huge Yannick Wattenberg: Assignment 04}\\
      \quad\newline
      ywattenberg@inf.ethz.ch, 19-947-464.\\
      \quad\newline
    \timestamp

\end{center}
\section*{Question 1: Calculating Prefix Probabilities}
\subsection*{a)}
We prove by 
The following equality is given
\begin{align}
    \sum_{w \in \Sigma} p(w) = 1 \label{eq:g1}
\end{align}
We first prove the following equivalence:
\begin{align}
    \sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w) = 1
\end{align}
Using induction over n and the above equation as induction hypothesis.
\begin{align}
    \sum_{w \in \Sigma^*, |w| = 1} \tilde{p}(w) &= \sum_{w \in \Sigma} p(w) = 1\\
    n &\rightarrow n+1 \nonumber\\
    \sum_{w \in \Sigma^*, |w| = n+1} \tilde{p}(w) &=  \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w' \circ w)\\
    &= \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} p(w')\tilde{p}(w) &&\text{def. }\tilde{p}\\
    &= \sum_{w' \in \Sigma}  p(w') \cdot \sum_{w \in \Sigma^*, |w| = n}\tilde{p}(w)\\
    &= \sum_{w' \in \Sigma}  p(w') &&(IH)\\
    &= 1 &&\ref{eq:g1}
\end{align}

With this we can reformulate the sum as follows:
\begin{align}
    \sum_{w \in \Sigma^*} \tilde{p}(w) &= \sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\\
    &=   \sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\\
    &=   \sum_{i=0}^{\infty} 1\\
     \sum_{i=1}^{\infty} 1 &\rightarrow \infty
\end{align}

\subsection*{b)}
The following equality is given
\begin{align}
    \sum_{w \in \Sigma \cup EOS} p(w) = 1 \Rightarrow \sum_{w \in \Sigma} p(w) = 1 - p(EOS) \label{eq:g2}
\end{align}
We first prove the following equivalence:
\begin{align}
    \sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w) = (1 - p(EOS))^n
\end{align}
Using induction over n and the above equation as induction hypothesis.
\begin{align}
    \sum_{w \in \Sigma^*, |w| = 1} \tilde{p}(w) &= \sum_{w \in \Sigma} p(w) = 1 - p(EOS) &&\ref{eq:g2} \\
    n &\rightarrow n+1 \nonumber\\
    \sum_{w \in \Sigma^*, |w| = n+1} \tilde{p}(w) &=  \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} \tilde{p}(w' \circ w)\\
    &= \sum_{w' \in \Sigma}\sum_{w \in \Sigma^*, |w| = n} p(w')\tilde{p}(w) &&\text{def. }\tilde{p}\\
    &= \sum_{w' \in \Sigma}  p(w') \cdot \sum_{w \in \Sigma^*, |w| = n}\tilde{p}(w)\\
    &= \sum_{w' \in \Sigma}  p(w') \cdot (1-p(EOS))^n &&(IH)\\
    &=  (1-p(EOS)) \cdot (1-p(EOS))^n &&\ref{eq:g2}\\
    &= (1-p(EOS))^{n+1}
\end{align}

With this we can reformulate the sum as follows:
\begin{align}
    \sum_{w \in \Sigma^*} p(w) = \sum_{w \in \Sigma^*} p(EOS) \tilde{p}(w)  &= p(EOS) \sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\\
    &= p(EOS)  \left(\sum_{i=0}^{\infty} \sum_{w \in \Sigma^*, |w| = i} \tilde{p}(w)\right)\\
    &= p(EOS)  \left(\sum_{i=0}^{\infty} (1-p(EOS))^{i} \right)\\
     p(EOS) \left(  \sum_{i=0}^{\infty} (1-p(EOS))^{i} \right) &\rightarrow p(EOS)  \frac{1}{p(EOS)} = 1 \quad (\text{geom. series})
\end{align}

\subsection*{c)}
\begin{align}
    p_{pre}(w) &\stackrel{!}{=} \sum_{u\in\Sigma^*}p(wu)\\
    \sum_{u\in\Sigma^*}p(wu) &= \sum_{u \in \Sigma^*}p(EOS|wu)p_{pre}(u|w)p_{pre}(w)\\
    &= p_{pre}(w) \left(\sum_{u \in \Sigma^*}p(EOS|wu)p_{pre}(u|w)\right)\\
    &= \frac{1}{p(EOS|w)} p(w) \left(\sum_{u \in \Sigma^*}p(u|w)\right) &&(\text{def.} P)\\
    &= \frac{1}{p(EOS|w)} \left(\sum_{u \in \Sigma^*}p(w)\frac{p(w|u)p(u)}{p(w)}\right) &&(\text{Bayes.}) \label{eq:bayes}\\
    &= \frac{1}{p(EOS|w)} \left(\sum_{u \in \Sigma^*}p(w|u)p(u)\right) \\
    &= \frac{1}{p(EOS|w)} p(w) &&(\text{Bayes.})\\
    &= p_{pre}(w) &&(\text{def.} P)
\end{align}
In (\ref{eq:bayes}) we apply baysens rule which holds as the sum iterates over all possible suffixes. This means we sum over the whole probability space which gives probability one for the event we condition on.

\subsection*{d)}
One can use the normal CKY algorithm where we define the score-function as the natural logarithm of the probability of the applied rule.
This means the CKY algorithm will calculate
\begin{align}
   chart[i,k,X] \mathrel{+}= &exp(score(X\rightarrow Y Z)) \cdot chart[i,j,Y] \cdot chart[j,k,Z] \\
    = &p( Y Z| X) \cdot p(Y|w_i,...,w_j) \cdot p(Z|w_j,...,w_k)\\
    \Rightarrow chart[i,k,X] = &p(X|w_i,...,w_k)
\end{align}
Which gives us $p(w_1,...,w_n|S)$ for the top most cell. This is the Prefix probability of the sentence we can multiply this by $p(EOS|S)$ to get the probability of the sentence.

\subsection*{e)}
\begin{align}
    \sum_{u \in \Sigma^*}p(wu) &\stackrel{!}{=} p(S \stackrel{*}{\Rightarrow} wv)\\
    \sum_{u \in \Sigma^*}p(wu) &= \sum_{u \in \Sigma^*} p_{inside}(wu|S) &&(\text{S is the starting symbol})\\
    &= \sum_{u \in \Sigma^*} p(S \stackrel{*}{\Rightarrow} wu) &&(\text{def. }p_{inside})\\
    &= p(S \stackrel{*}{\Rightarrow} wv) &&(\text{for some arbitrary }v \in \Sigma^*)
\end{align}
In the last step we replace the sum by some arbitrary suffix $v$. This is possible as the sum iterates over all possible suffixes and ($\stackrel{!}{\Rightarrow}$) produces all possible derivations meaning it also generates all possible suffixes.

\subsection*{f)}
First we define the probability matrix $W$ of size $|\mathcal{N}| \times |\mathcal{N}|$ as follows: Every entry $W[A,B]$ gets the probability of $p[A \Rightarrow B\alpha]$ where $\alpha$ is the rest of the rule. We can calculate this probability by adding the probabilities of all rules which produce $B$ as a first symbol. There can be at most $|\mathcal{N}|$ such rules as our grammar is in Chomsky normal form.
This means it is possible to calculate all entries of the matrix $W$ in $\mathcal{O}(|\mathcal{N}|^3)$.

Now we have to calculate $W^*$. We can do this using Lehmann's algorithm as seen in the lecture notes. For this to work it has to hold that entries on the diagonal of $W$ are smaller than one, otherwise $W[A,A]^*$ would be infinity/diverge. However all entries on the diagonal have to be smaller than one as otherwise this would mean all derivations from $A$ produce another $A$ thus every sentence would be infinite, which we assume to not be the case. So we can use Lehmann's algorithm to calculate $W^*$ which runs in $\mathcal{O}(|\mathcal{N}|^3)$.
Now the entry $W^*[A,B]$ is the probability of $p[A  \stackrel{*}{\Rightarrow} B\alpha] = p_{lc}(B|A)$. This follows from the definition of $W^*$ and the definition of the matrix product. We already proved in Assignment 3 exercise 2 b) that $M^n$ encodes the sum of path lengths of length $n$ in the graph $M$, this translates to our example where the path length will be the probabilities of a specific derivation path.
Then it follows from the definition of $W^*$ that $W^*[A,B]$ is the sum of probabilities of a derivation path from $A$ to $B$ of length $n$ for $n$ from $0$ to $\infty$. This is the same as $p_{lc}(B|A)$.
Now with all $p_{lc}(B|A)$ calculated we can calculate $p_{lc}(YZ|X) = \sum_{X' \in \mathcal{N}} p_{lc}(X'|X)p(X' \Rightarrow YZ)$ which is in $\mathcal{O}(|\mathcal{N}|)$. Doing this for all entries will take $\mathcal{O}(|\mathcal{N}|^4)$ as we have to calculate the sum for all possible $X$, $Y$ and $Z$.

\subsection*{g)}
\begin{align*}
    p_{pre}(w_i,...,w_k|X) \stackrel{!}{=} \sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)\cdot p_{inside}(w_i,...,w_j|Y)\cdot p_{pre}(w_j+1,...,w_k|Z)
\end{align*}
\begin{align}
    &\sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p_{lc}(YZ|X)\cdot p_{inside}(w_i,...,w_j|Y)\cdot p_{pre}(w_j+1,...,w_k|Z)\\
    =&\sum_{j=1}^{k-1}\sum_{Y,Z\in\mathcal{N}}p(X \stackrel{*}{\Rightarrow} YZ\alpha)\cdot p(Y \stackrel{*}{\Rightarrow} w_i,...,w_j)\cdot p(Z \stackrel{*}{\Rightarrow} w_{j+1},...,w_k,\textbf{v})\\
    =&p(X \stackrel{*}{\Rightarrow} w_i,...,w_k,\textbf{v})\label{eq:big} \\
    =&p_{pre}(w_i,...,w_k|X)
\end{align}
The important step here is (\ref{eq:big}) the step to this equation holds as we some 
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
